\section{本文提出的算法}
% \miniframesoff

\subsection{问题和假设}
\frame
{
\frametitle{问题和假设}
\begin{block}{问题}
有限和形式的凸优化问题：
$\underset{w \in \mathbb{R}^d}{\text{min}}\ F(w) = \frac{1}{N}\sum_{i=1}^{N} f_i(w)$
\end{block}

\pause

\begin{block}{假设}
\begin{itemize}
    \item \textbf{（Lipschitz光滑假设）}\
    $\forall i \in \{1, \ldots, N\}, \forall w_1, w_2 \in \mathbb{R}^d$,
    $$
    \| \nabla f_i(w_1) - \nabla f_i(w_2) \| \le L \| w_1 - w_2 \|.
    $$

    \item \textbf{（强凸假设）}\
	$F(w)$是$\mu$-强凸函数，即对任意的$w_1, w_2 \in \mathbb{R}^d$，
    $$
	    (\nabla F(w_1) - \nabla F(w_2))^T(w_1 - w_2) \ge \mu \| w_1 - w_2 \|^2.
    $$

    \item 定义问题的条件数为$\kappa = L / \mu$

\end{itemize}
\end{block}
}

\frame
{
\frametitle{应用例子}
\begin{itemize}
    \item 经验风险最小化问题：带$l_2$惩罚项的最小二乘、逻辑回归、光滑SVM……
    $$
        F(w) = \frac{1}{N}\sum_{i=1}^N\phi(w^\top x_i, y_i) + \frac{\mu}{2}\|w\|^2
    $$
    \item 非凸优化的重要子问题：
    $$
	\min_{w} \frac{1}{N}\sum_{i=1}^{N}\phi_i(w) + \frac{L}{2}\|w - \tilde{w}\|^2
    $$

\end{itemize}
}

\subsection{SIG算法}
\frame
{
\frametitle{\subsecname~ }
基于快照的确定性增量梯度（Snapshot-based Incremental Gradient，简称 SIG）方法：

{
\setlength{\interspacetitleruled}{-.4pt}%
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
\SetKwInOut{Return}{return}
\SetAlgoLined
\Input{$\eta$, $w_0^0$, S} \
\For{$s = 0, 1, \ldots, S-1$}
{
    \alert<2>{compute $G = \frac{1}{N} \sum_{i=1}^{N} \nabla f_i(w_0^s)$} \

    \For{$t = 0, \ldots, N-1$}
    {
    \alert<3>{$i_t = t+1$} \

    %$i_t = \sigma^s(i+1)$, $\sigma^s$ can be any peretatation on $\{1,\ldots,N\}$\

    $w_{t+1}^{s} = w_t^s - \eta \big(\alert<4>{\nabla f_{i_t}(w_t^s) - \nabla f_{i_t}(w_0^s) + G} \big)$ \

    }
    $w_0^{s+1} = w_N^s$ \
}
\Return{$w_0^S$}
\end{algorithm}
}

}

\frame
{
\footnotesize
\frametitle{\subsecname~ }
\begin{block}{SIG算法的特点}
    \begin{itemize}
        \item 以确定性的顺序访问数据
        \item 不需要维护历史的梯度信息
        \item 在光滑和强凸假设下达到线性收敛
    \end{itemize}
\end{block}

\pause

\begin{block}{与SVRG\uncover<2->{\footnote{Johnson \& Zhang, Accelerating stochastic gradient descent using predictive variance reduction. NIPS 2013.}}算法的关系}
    \begin{itemize}
        \item 使用几乎相同的梯度估计量
        \item SVRG：随机访问样本数据，SIG：循环顺序访问
        \item SVRG：内循环长度依赖于问题的条件数，SIG：把样本数作为内循环长度
    \end{itemize}
\end{block}
}

\frame
{
\frametitle{与现有方法的比较}
\footnotesize
\begin{table}[H]
	\centering
	\caption{SGD、SVRG、 SAGA、 DIG、 IAG和SIG方法的比较}
	\label{table:methods}
	\begin{tabular}{|c|c|c|c|c|c|>{\columncolor[gray]{0.8}}c|}
		\hline
		算法 & SGD & SVRG & SAGA & DIG & IAG & SIG \\ \hline
		\begin{tabular}[c]{@{}c@{}}数据访问方式\\  (读/写)\end{tabular}              & 只读   & 只读    & 读+写  & 只读  & 读+写 & 只读   \\ \hline
		\begin{tabular}[c]{@{}c@{}}组成函数选择方式 \\ (随机/确定性)\end{tabular} & 随机   & 确定性    & 随机性    & 确定性  & 确定性   & 确定性   \\ \hline
		存储复杂度 & $\mathcal{O}(d)$ & $\mathcal{O}(d)$ & $\mathcal{O}(Nd)$ & $\mathcal{O}(d)$ & $\mathcal{O}(Nd)$ & $\mathcal{O}(d)$ \\ \hline
		线性收敛？ & $\times$  & \checkmark  &  \checkmark  & $\times$ & \checkmark & \checkmark \\ \hline
	\end{tabular}
\end{table}

}

\subsection{SIG-M算法}
\frame
{
% \footnotesize
SIG-M：引入动量（Momentum）\footnote{Allen-Zhu, Katyusha: The first direct acceleration of stochastic gradient methods. ACM STOC, 2017.}来加速SIG的收敛

\pause

\begin{algorithm}[H]
  \SetKwInOut{Input}{input}
  \SetKwInOut{Return}{return}
  \SetAlgoLined
  \Input{$\alpha$, $w_0^0$, $S$, $\tau_1$, $\tau_2$} \
  $y_0^0 = z_0^0 \leftarrow w_0^0$; \

  \For{$s = 0, 1, \ldots, S-1$}
  {
   compute $G = \frac{1}{N} \sum_{i=1}^{N} \nabla f_i(w_0^s)$ \

   \For{$t = 0, \ldots, N-1$}
   {
    $i_t = t+1$ \

    %$i_t = \sigma^s(i+1)$, $\sigma^s$ can be any peretatation on $\{1,\ldots,N\}$\

    \alert<4>{$w_{t+1}^{s} = \tau_1 z_t^s + \tau_2 w_0^s + (1 - \tau_1 - \tau_2) y_t^s$} \

    \alert<3>{$z_{t+1}^s = z_t^s - \alpha \big(\nabla f_{i_t}(w_t^s) - \nabla f_{i_t}(w_0^s) + G \big)$} \

    \alert<3>{$y_{t+1}^s = w_{t+1}^s + \tau_1 (z_{t+1}^s - z_t^s)$} \
    % $y_{t+1}^s = w_{t+1}^s - \frac{1}{3 L} \big(\nabla f_{i_t}(w_t^s) - \nabla f_{i_t}(w_0^s) + G \big) $ \

   }
   $w_0^{s+1} = w_N^s$, $y_0^{s+1} = y_N^s$, $z_0^{s+1} = z_N^s$ \

  }
  \Return{$w_0^S$}
\end{algorithm}
}



% \miniframeson
